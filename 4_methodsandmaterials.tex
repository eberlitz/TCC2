% As this work, aims to compare techniques of word similarity based on two approaches,  lexical bases and distributional, we did some evaluation on 
% use a common portuguese dataset for evaluation called PT65 and 
% In order to be able to generate the word embeddings using multiple algorithms, it is required a corpus. For this, we got a portuguese Wikipedia dump and did some pre-processing to generate a corpus suitable for the given task.
% This work aims to compare techniques based on lexical bases with the distributional based ones.

% - Explore the existing techniques regarding word similarity, using a distributional approach called word embeddings, adapting existing works to Brazilian Portuguese.
% - Compare the word embeddings approach to other techniques that are solely based on a lexical database such as WordNet.
% - Adapt existing studies regarding the addition of syntactic context in the training process of word embeddings to a Brazilian Portuguese corpus, to check if the results will be, similar or not. 
% - Evaluate the different techniques over a common \textit{dataset}.

% \subsubsection{WordSimilarity-353 Test Collection}
% The WordSimilarity-353 is a test collection for measuring word similarity developed by \citetexto{Finkelstein:2001:PSC:371920.372094}. This dataset contains two sets of English word pairs along with human-assigned similarity judgements.
% http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/
% \subsubsection{ENSEPRO Word Similarity}
% Aiming to prove an improvement in the accuracy of information extraction systems, a specific dataset will be assembled using the questions and answers of short sentences used in the ENSEPRO system. This system evaluation was made using the dataset for the challenge Question Answering over Linked Data (QALD-7) of 2017, which also had the addition of the Portuguese language. For each sentence of the Portuguese questions of this dataset, we will assemble a new dataset with only the keywords necessary to carry out queries via the ENSEPRO search engine. A pair of this keyword must be generated with the one that is currently in the linked data. For words that the system currently cannot find answers due to lack of a word in the expansion of related terms, it will be investigated manually which would be the word that should be found and added to the dataset. Also, the dataset will include the syntactic information for each keyword.



\section{Methods and materials}\label{chap:methodsandmaterials}

This chapter will present a description of what and how this work was done, as well as the tools and methods used. First ~\autoref{chap:methodsandmaterials:architecture} presents an overview of the architecture and how the proposed experiment was realized. Then, the ~\autoref{chap:methodsandmaterials:dataset} presents the dataset used in the evaluation process. After that we start with a detailed explanation of the Corpus generation in ~\autoref{chap:methodsandmaterials:corpus}, of the syntactic parsing in ~\autoref{chap:methodsandmaterials:palavras}. Then we explain how we generate the most common word embedding models in ~\autoref{chap:methodsandmaterials:wegeneration} and at last we explain how we reproduced the work of \citetexto{Levy2014} for Portuguese in ~\autoref{chap:methodsandmaterials:depsgeneration}.

\subsection{Architecture overview}\label{chap:methodsandmaterials:architecture}

The proposed work basically consists of comparing different word similarity techniques. Therefore, ~\autoref{fig:arq} defines an overview of the architecture with the intention of comparing several techniques using different algorithms and testing them with a common dataset.

\begin{figure}[h]
	\caption{Proposed architecture}
	\label{fig:arq}
	\centering%
	\begin{minipage}{.8\textwidth}
		\includegraphics[width=\textwidth]{arq.png}
		\fonte{Made by the author.}
	\end{minipage}
\end{figure}


In this work we compared techniques based on the two main approaches to word similarity, the knowledge-based and the distributional-based. 

Regarding the knowledge-based approach we utilized a lexical base, in this case, \textbf{Open Multilingual Wordnet} (OMW) wore used with \textbf{Path Distance} and \textbf{Wu-Palmer} similarity techniques. It was decided to use OMW due to its ease of use through the \textbf{Natural Language Toolkit} (NLTK) library available for the \textbf{Python version 3.6} programming language as well as the availability of the Portuguese language for querying the synsets. \cite{Bond2013LinkingAE}.

For the distributional approach we \textbf{generated word embedding models} with a corpus obtained from the \textbf{brazilian portuguese Wikipedia dump} of articles. The word embeddings wore generated using several diferent model implementations for learning word representations. In this case, \textbf{FastText}, \textbf{Wang2vec}, \textbf{Word2vec} and \textbf{GloVe}. Also, we compare our word embedding models with a set of pre-trained models available from \textbf{Núcleo Interinstitucional de Linguística Computacional} (NILC) in all different implementations (FastText, Wang2vec, Word2vec and GloVe). One thing to note is that, the metric used for the comparison of similarity between one word and another for all word embedding models was the \textbf{cosine distance}. The \textbf{CBOW} and \textbf{Skip-gram} were used for the models that has this option. \cite{bojanowski2016enriching,Ling:2015:naacl,Mikolov2013DistributedRO,Pennington2014,Hartmann2017}

We also generated one more model in order to take into account the syntactic tree information of the sentences from the Portuguese corpus using the algorithm implementation by \citetext{Levy2014} wich generates the \textbf{DEPS} model. In order to do this, we used the \textbf{PALAVRAS syntactic parser} to annotate the corpus with syntactic information. \cite{bick2000palavras}.

In the end we do a quantitative evaluation of all models and techniques using the \textbf{PT65} dataset, which consists in a pair of words and a similarity value given by persons. \cite{GranadaSV14}.

All the experiments were done using the \textit{Semantics} computer (Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz with 32 cores and 128GB of RAM) granted by the \textit{UNISINOS Programa de Pós-graduação em Computação Aplicada} (PPGCA) by running \textbf{Docker} containers.


\subsection{PT65 Dataset}\label{chap:methodsandmaterials:dataset}

This dataset is composed by 65 word pairs, initialy generated by \citetexto{Rubenstein1965ContextualCO} on the name of \textit{RG65}. This word pairs wore translated to portuguese by \citetexto{GranadaSV14} and evaluated with 50 persons. 

The initial idea was to use the WordSimilarity-353 Test Collection developed by \citetexto{Finkelstein:2001:PSC:371920.372094} which consists of two sets of English word pairs along with human-assigned similarity judgements. But we would have to translate to portuguese and then the human-assigned similarity judgements would not fit entirelly regarding the semantic changes involved in the translation proccess. So the PT65 dataset was used in the evaluation process.

\input{4_corpus}








% CÓDIGO: https://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings/
% DATASET: http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/

% resources:
% - Open Multilingual Wordnet
% - NILC Pre-trained word embeddings models
% Corpus:
%  - Pegar com o allan o corpus portugues
%  - Anotar o corpus com o PALAVRAS syntactic parser
% quantitative evaluation datasets:
%  - WordSim353 manual translation to portuguese
%  - ENSEPRO word similarity (vou criar em conjunto com o denis, preciso definir formato e como, usando as perguntas do QALD)
% Algoritmo:
%  - Usar o código fonte do DEPS
% tools:
%  - Palavras
%  - Python NLTK, word2vec, word2vecf, 













% \begin{itemize}
%     \item Explore the existing techniques regarding word similarity, using a distributional approach called word embeddings, adapting existing works to Brazilian Portuguese.
%     \item Compare the word embeddings approach to other techniques that are solely based on a lexical database such as WordNet.
%     \item Adapt existing studies regarding the addition of syntactic context in the training process of word embeddings to a Brazilian Portuguese corpus, to check if the results will be, similar or not. 
%     \item Evaluate the different techniques over a common \textit{dataset} related to \citetexto{denis2018} work.
% \end{itemize}

% This thesis is structured as follows. The \autoref{chap:background} presents the general concepts and techniques used in this work. In \autoref{chap:relatedwork} are described and analyzed the works related to the research area of this work. The \autoref{chap:methodsandmaterials} presents the proposed model, as well as the form of the experiment and the necessary tools. The \autoref{chap:results} presents the preliminary results obtained in the case study experiment. Finally, \autoref{chap:conclusions} summarizes the thesis findings, contributions, and discusses.





% Descrever um cenário em que o projeto se faz necessário (Extração de informação em ontologias utilizando Linguagem Natural - Talvez citar o uso de queries que utilizem triplas RDF) e ajuda a resolver o problema. Após descrito citar como exemplo o trabalho do denis e detalhar o cenário dele.

% - Cenário do denis: Recebe uma pergunta; Realiza um parse, extraindo palavras relevantes e seu contexto sintático; Realiza a geração de combinações diferentes de possíveis tuplas RDF de pesquisa; Algumas dessas pesquisas não se encontra a resposta, porém se uma destes elementos da tupla fosse substituído por um outro sinônimo ele iria encontrar a resposta. Portanto é usado o WordNet BR para procurar por sinônimos, porém ainda assim, nem sempre se tem a palavra cadastrada no WordNet.

% Descrever a arquitetura do modelo proposto.

% - Usar o NLTK python e procurar por sinônimos de uma dada palavra no OpenWordNet-PT ( supõe-se que algumas vezes não será encontrado nada )
% - Usar diferentes tipos de word embedding (BoW, SG, GloVe)
% - [TCC2]Treinar word embeddings sobre um corpus (pegar com o allan) utilizando o contexto gerado pelo POS Tagger da unisinos (Aonde consigo??)
% - [TCC2]Algum outro algoritmo se houver necessidade
% - Testar cada abordagem com o dataset do denis




% OpenWordNet-PT. \cite{coling2012}.

