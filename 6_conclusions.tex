\section{Final considerations}\label{chap:conclusions}

The study carried out in this work, indicates that the detection of similarity between words is a very important topic for the NLP segments as summarization, information retrieval, and question answering. Techniques for identifying word similarity can help in a number of NLP tasks such as dialogue systems, question answering, and information retrieval systems. \cite{Islam2007ApplicationsOC,Pilehvar2013,Agirre2009}.

It was also found indications that the expansion of terms using WordNet has several problems, where a word may not be present. Since these lexical bases are of manual construction, they are time-consuming and expensive, and for this reason, not all links will be present and their quality varies from language to language.  There is also no WordNet for all languages. \cite{Leeuwenberga2016}. Distributional approaches regarding word similarity have been proven to be more competitive than the thesaurus-based approach, and have been successfully being used to cover out-of-vocabulary items in WordNet.  \cite{Agirre2009}.

% Thus, WordNet is proposed to be replaced by Word embeddings, which follows a distributional approach and therefore does not depend on manual construction, and can be applied to different languages since its training is unsupervised. Thus, the hypothesis is that for the formulation of queries in QA and IR systems on which they depend on the expansion of similar terms it would be possible to increase the number of relevant results to be found.

Thus, we explored the existing techniques regarding word similarity, using a distributional approach called word embeddings, adapting existing works to Brazilian Portuguese. We also experiment with other techniques that are solely based on a lexical database such as WordNet, and we evaluate all the different techniques over a common dataset called PT65. Therefore indicating that word embeddings can cover words out of vocabulary and have slightly better results in comparison with WordNet. We also adapted the studies of \citetexto{Levy2014} regarding the addition of syntactic context in the training process of word embeddings to a Brazilian Portuguese corpus, finding similar results, but for the task of word similarity against the dataset PT65, it had the worst results.

As a future work, we intend to evaluate the different techniques against the work of \citetexto{denis2018} to see if we can improve the ENSEPRO results. As well as to explore how we could use BERT and ELMo language models for word-level tasks such as word similarity as these works represent the state-of-the-art.