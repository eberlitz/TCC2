\section{Results}\label{chap:results}


We separated the evaluation in three steps. In \autoref{chap:results:wn} we present a quantitative evaluating of the Open Multilingual WordNet. In \autoref{chap:results:we} we present the same evaluation but with the word embeddings models. At last, we present a qualitative evaluation regarding the DEPS model in \autoref{chap:results:qualitative}

% For the evaluation, we propose to do qualitative measure based on a manual inspection of the most similar words to a hand picked set of words and see if they are in fact similar or not to the other words regarding similarity, relatedness and also the syntactic similarity.
% Also a quantitative analyses is proposed by using the WordSim353 dataset from \citetexto{Finkelstein:2001:PSC:371920.372094}, which is a dataset regarding word similarity versus relatedness. As all the pairs in this dataset are in English, they will be manually translated to Portuguese to be able to compare the results with all the techniques.
% To better visualize the performance of each technique the precision-recall curve draws will be used to describes the embeddings affinity.
% Another quantitative analysis will be done to see if the recall increase using the ENSEPRO dataset. If it increases it will improve the quality of the ENSEPRO system. 


\subsection{Open Multilingual WordNet evaluation}\label{chap:results:wn}

In order to do a quantitative evaluation of the knowledge-based approach for word similarity, we used the Open Multilingual Wordnet (OMW) from \citetexto{Bond2013LinkingAE} and loaded it with the Natural Language Toolkit (NLTK) library. We then calculated the similarity between the pair of words from the PT65 dataset using two algorithms, Path Distance and Wu-Palmer. With this, we calculated the Pearson’s Correlation ($\rho$) for each of the techniques.
\autoref{tab:worneteval} shows the results, and as we can see, the Path Distance algorithm gave a relatively high score, but as stated by \citetexto[p.~297]{Jurafsky:2009:SLP:1214993} we indeed have the occurrence of some out of vocabulary words, in this case, 15.38\% of the words.


\begin{table}[h]
    \caption{OMW evaluation on PT65. \textbf{$r$} is the Pearson’s Correlation considering only the words in vocabulary; \textbf{$\rho$} is the Pearson’s Correlation considering all the words, given a similarity value of zero for words out of vocabulary. The higher value is in bold for better readability.
    }
    \label{tab:worneteval}
    \centering%
    \begin{minipage}{.6\textwidth}
    \begin{tabular}{@{}lllll@{}}
    \cmidrule(r){1-5}
    \textbf{Algorithms} & \textbf{$r$} & \textbf{$\rho$}         & \textbf{Out of vocabulary ratio} \\ 
    \cmidrule(r){1-4}
    Path Distance & \textbf{0.76} & 0.67   & 15.38                   \\
    Wu-Palmer     & 0.62    & 0.51         & 15.38                   \\ \cmidrule(r){1-4}
    \end{tabular}
    \fonte{Made by the author.}
    \end{minipage}
\end{table}

\subsection{Word embeddings Evaluation}\label{chap:results:we}

To do a quantitative evaluation of the distributional approach for word similarity we did the same experiment as the WordNet evaluation but with our word embeddings models.
We loaded the PT65 dataset, and for each word pair, we compared the expected result with the Cosine similarity given by the model. With this, we calculated the Pearson’s Correlation ($\rho$).
\autoref{tab:evaluation:we} shows the results for all the 40 generated models. There was no out of vocabulary words in this approach which in comparison with the WordNet approach is better. Just like mentioned by \citetexto{Agirre2009} we can use word embeddings to cover out-of-vocabulary words. In comparison with the WordNet, we can also see, that it has slightly better results, which is a good thing considering that it has no manual construction as WordNet.


\begin{table}[]
\caption{Word embeddings evaluation on PT65. \textbf{$\rho(ours)$} is the Pearson’s Correlation value from our trained models. \textbf{$\rho(nilc)$} is the Pearson’s Correlation values from the NILC pre-trained models. All values equals or greater than 0.75 are in bold for better readability.}
\label{tab:evaluation:we}
\centering%
\begin{minipage}{.65\textwidth}
\begin{tabular}{@{}lllll@{}}
\cmidrule(r){1-5}
\textbf{Embedding Models} &      & \textbf{Size} & \textbf{$\rho(ours)$} & \textbf{$\rho(nilc)$} \\ \cmidrule(r){1-5}
FastText            & CBOW          & 50   & 0.67             & 0.63            \\
                    &               & 100  & 0.72             & 0.67            \\
                    &               & 300  & \textbf{0.75}    & 0.73            \\
                    &               & 600  & 0.73             & 0.74            \\
                    &               & 1000 & 0.71             & 0.74            \\ \cmidrule(lr){2-5}
                    & Skip-Gram     & 50   & 0.74             & 0.64            \\
                    &               & 100  & \textbf{0.77}    & 0.73            \\
                    &               & 300  & \textbf{0.79}    & \textbf{0.78}   \\
                    &               & 600  & \textbf{0.77}    & \textbf{0.76}   \\
                    &               & 1000 & 0.72             & 0.74            \\ \cmidrule(r){1-5}
Wang2vec            & CBOW          & 50   & 0.57             & 0.59            \\
                    &               & 100  & 0.61             & 0.69            \\
                    &               & 300  & 0.69             & 0.74            \\
                    &               & 600  & 0.69             & 0.66            \\
                    &               & 1000 & 0.68             & 0.65            \\ \cmidrule(lr){2-5}
                    & Skip-Gram     & 50   & 0.65             & 0.60            \\
                    &               & 100  & \textbf{0.74}    & 0.70            \\
                    &               & 300  & \textbf{0.75}    & \textbf{0.77}   \\
                    &               & 600  & 0.72             & \textbf{0.76}   \\
                    &               & 1000 & 0.69             & 0.71            \\ \cmidrule(r){1-5}
Word2vec            & CBOW          & 50   & 0.58             & 0.34            \\
                    &               & 100  & 0.63             & 0.43            \\
                    &               & 300  & 0.68             & 0.58            \\
                    &               & 600  & 0.69             & 0.62            \\
                    &               & 1000 & 0.68             & 0.61            \\ \cmidrule(lr){2-5}
                    & Skip-Gram     & 50   & 0.65             & 0.48            \\
                    &               & 100  & \textbf{0.75}    & 0.54            \\
                    &               & 300  & \textbf{0.76}    & 0.64            \\
                    &               & 600  & 0.74             & 0.68            \\
                    &               & 1000 & 0.69             & 0.67            \\ \cmidrule(r){1-5}
GloVe               &               & 50   & 0.63             & 0.63            \\
                    &               & 100  & 0.69             & 0.71            \\
                    &               & 300  & 0.69             & 0.72            \\
                    &               & 600  & 0.67             & 0.71            \\
                    &               & 1000 & 0.65             & 0.68            \\ \cmidrule(r){1-5}
DEPS                &               & 50   & 0.47             \\
                    &               & 100  & 0.44             \\
                    &               & 300  & 0.43             \\
                    &               & 600  & 0.45             \\
                    &               & 1000 & 0.44             \\ \cmidrule(r){1-5}
\end{tabular}
\fonte{Made by the author.}
\end{minipage}
\end{table}

Also, we can see that the better word embedding model for this task is the FastText Skip-Gram. In all of them, Skip-gram was slightly better than the others. Moreover, in overall the models with 300-600 dimensions got higher values. We can also note that the DEPS model has an inferior performance in this particular task, maybe because the dataset does not differentiate between relatedness and similarity.
We also repeated the same experiment with the pre-trained models by \citetexto{Hartmann2017} from NILC.


\subsection{Qualitative Evaluation of DEPS model}\label{chap:results:qualitative}

For evaluating our DEPS model we did a qualitative evaluation were we manually inspect the five most similar words (by cosine similarity) to a given set of target words (\autoref{tab:qualitative}), and we compared it with other models, just like \citetexto{Levy2014} did in their experiment.


% \begin{landscape}
\begin{board}[h]
\caption{Target words and their five most similar words per word embedding models.}
\label{tab:qualitative}
\centering%
% \begin{minipage}{\textwidth}
% \makebox[\textwidth]{
\resizebox{\columnwidth}{!}{
\begin{tabular}{|l|l|l|l|l|l|}
\hline
\textbf{Target word} & \textbf{DEPS} & \textbf{FastText} & \textbf{Wang2vec} & \textbf{Word2vec} & \textbf{GloVe} \\ \hline
longe & \begin{tabular}[c]{@{}l@{}}perto\\ lá\\ abaixo\\ cá\\ debaixo\end{tabular} & \begin{tabular}[c]{@{}l@{}}próxi­mo\\ distante\\ afastado-se\\ afastada\\ afastados\end{tabular} & \begin{tabular}[c]{@{}l@{}}perto\\ distante\\ afastada\\ fora\\ distantes\end{tabular} & \begin{tabular}[c]{@{}l@{}}distante\\ fora\\ perto\\ afastada\\ tirá-los\end{tabular} & \begin{tabular}[c]{@{}l@{}}perto\\ fora\\ ficar\\ lá\\ tão\end{tabular} \\ \hline
guarda-chuva & \begin{tabular}[c]{@{}l@{}}caneta\\ tampão\\ espingarda\\ cetro\\ carregador\end{tabular} & \begin{tabular}[c]{@{}l@{}}guarda-chuvas\\ manda-chuva\\ manda-chuvas\\ guarda-chaves\\ guarda-copos\end{tabular} & \begin{tabular}[c]{@{}l@{}}lenço\\ sobre-tudo\\ quepe\\ moletom\\ pulôver\end{tabular} & \begin{tabular}[c]{@{}l@{}}galhardete\\ xale\\ chapeu\\ abajur\\ paletó\end{tabular} & \begin{tabular}[c]{@{}l@{}}égide\\ cinza\\ casaco\\ disfarce\\ crachá\end{tabular} \\ \hline
correr & \begin{tabular}[c]{@{}l@{}}viajar\\ aceitar\\ ganhar\\ aprender\\ realizar\end{tabular} & \begin{tabular}[c]{@{}l@{}}correndo\\ correrem\\ correu\\ correra\\ correria\end{tabular} & \begin{tabular}[c]{@{}l@{}}correndo\\ caminhar\\ pedalando\\ correu\\ agachar-se\end{tabular} & \begin{tabular}[c]{@{}l@{}}correndo\\ correu\\ caminhar\\ pedalando\\ pular\end{tabular} & \begin{tabular}[c]{@{}l@{}}caminhar\\ nadar\\ correndo\\ saltar\\ pular\end{tabular} \\ \hline
inglês & \begin{tabular}[c]{@{}l@{}}espanhol\\ francês\\ norueguês\\ sueco\\ italiano\end{tabular} & \begin{tabular}[c]{@{}l@{}}inglêss\\ inglêz\\ inglês-the\\ inglêsa\\ francês-inglês\end{tabular} & \begin{tabular}[c]{@{}l@{}}ingles\\ português\\ espanhol\\ francês\\ galês\end{tabular} & \begin{tabular}[c]{@{}l@{}}ingles\\ espanhol\\ francês\\ português\\ irlandês\end{tabular} & \begin{tabular}[c]{@{}l@{}}português\\ francês\\ inglesa\\ espanhol\\ britânico\end{tabular} \\ \hline
faculdade & \begin{tabular}[c]{@{}l@{}}universidade\\ escola\\ liceu\\ conservatório\\ colégio\end{tabular} & \begin{tabular}[c]{@{}l@{}}faculda\\ faculdad\\ ex-faculdade\\ universidade\\ faculde\end{tabular} & \begin{tabular}[c]{@{}l@{}}universidade\\ bacharelando-se\\ politécnica\\ pós-graduação\\ puc\end{tabular} & \begin{tabular}[c]{@{}l@{}}universidade\\ histórico-filosóficas\\ bacharelando-se\\ politécnica\\ puc-pr\end{tabular} & \begin{tabular}[c]{@{}l@{}}universidade\\ medicina\\ ciências\\ curso\\ usp\end{tabular} \\ \hline
\end{tabular}
}

\fonte{Made by the author.}
% \end{minipage}
\end{board}
% \end{landscape}

The first target word, \textit{longe} (Far), we can see similar results provided by all the different models. The word \textit{inglês} (English) have the same behavior. However, for some specific words, like \textit{faculdade} (College) we can see that the DEPS model returned other types of languages or colleges while the other models could just bring words related to the same domain. This is similar to the target word \textit{Hogwarts} from the \citetexto{Levy2014} work.
The other two words, \textit{guarda-chuva} (Umbrella) and \textit{correr} (Run), demonstrates that the DEPS model find other words with the same syntactic function (verb and noun) like a classifier, which in terms of semantic similarity or relatedness is not so good, just as we saw in the qualitative experiment (\autoref{tab:evaluation:we}) where the DEPS model had the worst results in that particular task for Portuguese.
